{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "id": "e2392796b2b01f68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pip install spark-nlp\n",
    "pip install xgboost"
   ],
   "id": "c1a05e3289100e0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "companies = spark.read.parquet('/dbfs/linkedin_train_data')\n",
    "display(companies)"
   ],
   "id": "164bca9a624a2eab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_location = \"dbfs:/user/hive/warehouse/syb2\"\n",
    "# Reading our Linkedin job listing scraping data\n",
    "jobs = spark.read.format(\"delta\").load(file_location)\n",
    "display(jobs)"
   ],
   "id": "18ee7be27b00c5f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###EDA and organization of the data\n",
   "id": "1b96d98c7f73b63e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import regexp_extract, col, trim\n",
    "\n",
    "# Adding new column for the applicant column only with numeric values\n",
    "jobs_cleaned = jobs.dropna()\n",
    "jobs_cleaned = jobs_cleaned.withColumn(\"applicants_count\", regexp_extract(col(\"applicants\"), r\"(\\d+)\", 1))\n",
    "jobs_cleaned = jobs_cleaned.withColumn(\"applicants_count\", col(\"applicants_count\").cast(\"int\"))\n",
    "\n",
    "# Removing rows with null values and blank strings in the specified fields\n",
    "fields_to_check = [\"applicants\", \"company\", \"industries\", \"time_posted\", \"job_function\"]\n",
    "for field in fields_to_check:\n",
    "    jobs_cleaned = jobs_cleaned.filter(trim(col(field)) != \"\")\n",
    "\n",
    "# Display cleaned data\n",
    "display(jobs_cleaned)\n",
    "print(jobs_cleaned.count())"
   ],
   "id": "839f2be801f84434"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###Cluster the Industries field",
   "id": "aaed63b29c8bf11f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Find the best k for K-means using Elbow Method",
   "id": "7d3d5750fb699947"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Word2Vec, Normalizer, VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for null values in the industries column\n",
    "print(\"Null values in companies industries column:\", companies.filter(col(\"industries\").isNull()).count())\n",
    "print(\"Null values in jobs_cleaned industries column:\", jobs_cleaned.filter(col(\"industries\").isNull()).count())\n",
    "\n",
    "# Drop rows with null values in the industries column\n",
    "companies = companies.dropna(subset=[\"industries\"])\n",
    "jobs_cleaned = jobs_cleaned.dropna(subset=[\"industries\"])\n",
    "\n",
    "# Tokenize the industries field for companies\n",
    "tokenizer_industries = Tokenizer(inputCol=\"industries\", outputCol=\"industries_words\")\n",
    "companies_tokenized = tokenizer_industries.transform(companies)\n",
    "\n",
    "# Generate embeddings with Word2Vec for companies\n",
    "word2vec_industries = Word2Vec(vectorSize=25, minCount=1, inputCol=\"industries_words\", outputCol=\"industries_embedding\")\n",
    "model_industries = word2vec_industries.fit(companies_tokenized)\n",
    "companies_embedded = model_industries.transform(companies_tokenized)\n",
    "\n",
    "# Use only the industries_embedding as features for clustering\n",
    "assembler = VectorAssembler(inputCols=[\"industries_embedding\"], outputCol=\"raw_features\")\n",
    "companies_features = assembler.transform(companies_embedded)\n",
    "\n",
    "# Normalize the features to simulate Cosine Similarity for K-means\n",
    "normalizer = Normalizer(inputCol=\"raw_features\", outputCol=\"features\", p=2.0)\n",
    "companies_features_normalized = normalizer.transform(companies_features)\n",
    "\n",
    "# Elbow Method to find optimal k\n",
    "cost = []\n",
    "k_values = range(2, 15)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol=\"features\")\n",
    "    model = kmeans.fit(companies_features_normalized)\n",
    "    cost.append(model.summary.trainingCost)\n",
    "\n",
    "# Plot the Elbow Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, cost, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal k\", fontsize=16)\n",
    "plt.xlabel(\"Number of Clusters (k)\", fontsize=14)\n",
    "plt.ylabel(\"Cost (Inertia)\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "34843e2ccae582cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Apply KMeans with chosen k",
   "id": "f679169546047b33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Choose the optimal k based on the elbow plot\n",
    "optimal_k = 7\n",
    "\n",
    "# Fit the final KMeans model on the companies table\n",
    "kmeans_final = KMeans(k=optimal_k, seed=1, featuresCol=\"features\")\n",
    "final_model = kmeans_final.fit(companies_features_normalized)\n",
    "\n",
    "# Now, predict clusters for the jobs_cleaned table\n",
    "# Tokenize the industries field for jobs_cleaned\n",
    "jobs_cleaned_tokenized = tokenizer_industries.transform(jobs_cleaned)\n",
    "\n",
    "# Generate embeddings with Word2Vec for jobs_cleaned\n",
    "jobs_cleaned_embedded = model_industries.transform(jobs_cleaned_tokenized)\n",
    "\n",
    "# Use only the industries_embedding as features for clustering\n",
    "jobs_features = assembler.transform(jobs_cleaned_embedded)\n",
    "\n",
    "# Normalize the features\n",
    "jobs_features_normalized = normalizer.transform(jobs_features)\n",
    "\n",
    "# Predict clusters for jobs_cleaned\n",
    "predictions = final_model.transform(jobs_features_normalized)\n",
    "predictions = predictions.withColumnRenamed(\"prediction\", \"industry_cluster\")\n",
    "\n",
    "display(predictions)"
   ],
   "id": "5940f2a61e198640"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Assign meaningful names to clusters",
   "id": "ef2197c72181bafa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import collect_list, col, explode, split, count, concat_ws, slice\n",
    "\n",
    "# Group by cluster and collect industries\n",
    "cluster_summary = predictions.groupBy(\"industry_cluster\").agg(collect_list(\"industries\").alias(\"industries_list\")).orderBy(\"industry_cluster\")\n",
    "# Explode industries_list to individual industries\n",
    "exploded_industries = cluster_summary.withColumn(\"industry\", explode(col(\"industries_list\")))\n",
    "# Split industries into individual terms (if comma-separated)\n",
    "exploded_terms = exploded_industries.withColumn(\"term\", explode(split(col(\"industry\"), \",\\\\s*\")))\n",
    "# Count the frequency of each term for each cluster\n",
    "industry_counts = exploded_terms.groupBy(\"industry_cluster\", \"term\").count().orderBy(\"industry_cluster\", \"count\", ascending=[True, False])\n",
    "\n",
    "# Extract top term for each cluster\n",
    "top_keywords = (\n",
    "    industry_counts\n",
    "    .groupBy(\"industry_cluster\")\n",
    "    .agg(collect_list(\"term\").alias(\"top_terms\"))\n",
    ")\n",
    "top_keywords = top_keywords.withColumn(\"top_terms\", slice(col(\"top_terms\"), 1, 1))\n",
    "# Concatenate the top terms into a single cluster name\n",
    "cluster_names_automated = top_keywords.withColumn(\"cluster_name\", concat_ws(\", \", col(\"top_terms\")))\n",
    "display(cluster_names_automated)\n",
    "# Join cluster names back to predictions\n",
    "predictions_with_names = predictions.join(cluster_names_automated.select(\"industry_cluster\", \"cluster_name\"), on=\"industry_cluster\", how=\"left\")"
   ],
   "id": "46dd465b86506446"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dimentionality reduction for cluster plotting",
   "id": "6ab20eee9d1253a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply PCA to reduce features to 2 dimensions\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(predictions_with_names)\n",
    "pca_result = pca_model.transform(predictions_with_names)\n",
    "pca_data = pca_result.select(\"pca_features\", \"industry_cluster\", \"cluster_name\").rdd.map(\n",
    "    lambda row: (row[\"pca_features\"][0], row[\"pca_features\"][1], row[\"industry_cluster\"], row[\"cluster_name\"])\n",
    ").collect()\n",
    "pca_df = pd.DataFrame(pca_data, columns=[\"pca_x\", \"pca_y\", \"cluster\", \"cluster_name\"])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(pca_df[\"pca_x\"], pca_df[\"pca_y\"], c=pca_df[\"cluster\"], cmap=\"tab10\", s=30)\n",
    "plt.title(\"PCA Visualization of Clusters\", fontsize=16)\n",
    "plt.xlabel(\"PCA Dimension 1\", fontsize=14)\n",
    "plt.ylabel(\"PCA Dimension 2\", fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate Clusters with Names\n",
    "unique_clusters = pca_df[[\"cluster\", \"cluster_name\"]].drop_duplicates().sort_values(\"cluster\")\n",
    "for cluster, name in unique_clusters.itertuples(index=False):\n",
    "    cluster_points = pca_df[pca_df[\"cluster\"] == cluster]\n",
    "    center_x, center_y = cluster_points[\"pca_x\"].mean(), cluster_points[\"pca_y\"].mean()\n",
    "    plt.text(center_x, center_y, name, fontsize=12, ha=\"center\", bbox=dict(facecolor=\"white\", alpha=0.8))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "predictions_with_names.display()"
   ],
   "id": "4b3a2a08068bd042"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Preparing the features for the model**",
   "id": "d784156d89189f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, regexp_extract, when, max\n",
    "def convert_to_minutes(df):\n",
    "    # Extract number and unit using regexp\n",
    "    df = df.withColumn('time_number',\n",
    "                      regexp_extract(col('time_posted'), r'(\\d+)', 1).cast('int'))\n",
    "    df = df.withColumn('time_unit',\n",
    "                      regexp_extract(col('time_posted'), r'(\\w+) ago', 1))\n",
    "\n",
    "    # Convert all times to minutes\n",
    "    df = df.withColumn('minutes_ago',\n",
    "        when(col('time_unit') == 'minutes', col('time_number'))\n",
    "        .when(col('time_unit') == 'hours', col('time_number') * 60)\n",
    "        .when(col('time_unit') == 'hour', col('time_number') * 60)\n",
    "        .when(col('time_unit') == 'days', col('time_number') * 60 * 24)\n",
    "        .when(col('time_unit') == 'day', col('time_number') * 60 * 24)\n",
    "        .when(col('time_unit') == 'weeks', col('time_number') * 60 * 24 * 7)\n",
    "        .when(col('time_unit') == 'week', col('time_number') * 60 * 24 * 7)\n",
    "        .when(col('time_unit') == 'months', col('time_number') * 60 * 24 * 7 * 30)\n",
    "        .when(col('time_unit') == 'month', col('time_number') * 60 * 24 * 7 * 30)\n",
    "        .when(col('time_unit') == 'year', col('time_number') * 60 * 24 * 7 * 30 * 12)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "records_with_minutes = convert_to_minutes(predictions_with_names)\n",
    "group_columns = [col_name for col_name in records_with_minutes.columns if col_name != \"minutes_ago\"]\n",
    "# Group by these columns and get the maximum `minutes_ago` for each group\n",
    "records_with_minutes = (\n",
    "    records_with_minutes.groupBy(*group_columns)\n",
    "    .agg(max(\"minutes_ago\").alias(\"minutes_ago\"))\n",
    ")\n",
    "display(records_with_minutes)"
   ],
   "id": "dff37e299a4f688"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###Normalization of applicants with important parameters\n",
    "\n",
    "\n",
    "number of applicants has a few problems:\n",
    "1. correlation with company status: size \\ market cap \\ followers - bigger companies with more followers will tend to have more exposure.\n",
    "2. number of applicants Missing not at random - amount of applicants is between 25 to 200 with under 25 as 25 and over 200 as 200.\n",
    "3. number of applicants Missing at Random with correlation to time posted\n",
    "4. time posted Missing not at random - we gather time only in the largest unit avalible (minute, hour, day, weeks...). if a post exceeds minutes we will count only hours etc.\n"
   ],
   "id": "8300b71d698fe1a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train the first model to predict number of applicants with normalization",
   "id": "57f84747b58de4a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the fraction of records for each model\n",
    "\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "original_count = records_with_minutes.count()\n",
    "records_with_minutes = records_with_minutes.filter(\n",
    "    col(\"applicants_count\").isNotNull() &\n",
    "    col(\"applicants_count\").cast(IntegerType()).isNotNull()\n",
    ")\n",
    "new_count = records_with_minutes.count()\n",
    "print(f\"Rows dropped: {original_count - new_count}\")\n",
    "\n",
    "# Set the fraction to 1/3 for the sampling\n",
    "applicants_normalization_fraction = 1 / 3\n",
    "applicants_prediction_fraction = 2/3\n",
    "\n",
    "# Take a random sample of 1/3 of the records\n",
    "applicants_normalization_data, applicants_prediction_data = records_with_minutes.randomSplit([applicants_normalization_fraction, applicants_prediction_fraction], seed=42)\n",
    "\n",
    "display(applicants_normalization_data)\n",
    "print(applicants_normalization_data.count())\n",
    "display(applicants_prediction_data)\n",
    "print(applicants_prediction_data.count())\n"
   ],
   "id": "77911a6b36bcf55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Count the record count of each company to use it as a \"company size\" normalization",
   "id": "59307c443a22145d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Group by 'company', count records, order by count in descending order, and show the result\n",
    "amount_of_jobs = records_with_minutes.groupBy(\"company\") \\\n",
    "    .agg(count(\"*\").alias(\"record_count\")) \\\n",
    "    .orderBy(col(\"record_count\").desc())\n",
    "amount_of_jobs.display()"
   ],
   "id": "7eac93700b26af12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finding best parameters for the **Random Forest** model using the elbow method",
   "id": "1bc37cdefcb1185"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, Bucketizer, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create bins for the \"applicants_count\" column\n",
    "def create_binned_features(df):\n",
    "    splits = [float('-inf'), 26, 50, 75, 100, 125, 150, 175, 199, float('inf')] # Dividing to bins and not to an actual number because of the MAR\\MNAR mechnism in the applicants count column\n",
    "    bucketizer = Bucketizer(\n",
    "        splits=splits,\n",
    "        inputCol=\"applicants_count\",\n",
    "        outputCol=\"applicants_count_bin\"\n",
    "    )\n",
    "    return bucketizer.transform(df)\n",
    "\n",
    "# Prepare features for the model\n",
    "def prepare_features(df, categorical_cols=[\"seniority_level\", \"employment_type\"],\n",
    "                    numeric_cols=[\"record_count\", \"minutes_ago\", \"industry_cluster\"]):\n",
    "    for col in categorical_cols + numeric_cols:\n",
    "        df = df.na.fill(0, subset=[col])\n",
    "\n",
    "    if \"bin_prediction\" in df.columns:\n",
    "        df = df.drop(\"bin_prediction\")\n",
    "\n",
    "    indexers = [\n",
    "        StringIndexer(\n",
    "            inputCol=col,\n",
    "            outputCol=f\"{col}_indexed\",\n",
    "            handleInvalid=\"keep\"\n",
    "        ) for col in categorical_cols\n",
    "    ]\n",
    "\n",
    "    for indexer in indexers:\n",
    "        df = indexer.fit(df).transform(df)\n",
    "\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCols=[f\"{col}_indexed\" for col in categorical_cols],\n",
    "        outputCols=[f\"{col}_encoded\" for col in categorical_cols]\n",
    "    )\n",
    "    df = encoder.fit(df).transform(df)\n",
    "\n",
    "    feature_cols = numeric_cols + [f\"{col}_encoded\" for col in categorical_cols]\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_cols,\n",
    "        outputCol=\"bin_prediction\"\n",
    "    )\n",
    "\n",
    "\n",
    "    return assembler.transform(df)\n",
    "\n",
    "def plot_elbow_curves(param_metrics):\n",
    "    \"\"\"Plot elbow curves for different parameters.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Parameter Tuning Elbow Curves')\n",
    "\n",
    "    # Plot numTrees\n",
    "    num_trees_df = param_metrics[param_metrics['maxDepth'] == param_metrics['maxDepth'].median()]\n",
    "    axes[0, 0].plot(num_trees_df['numTrees'], num_trees_df['metric'], marker='o')\n",
    "    axes[0, 0].set_xlabel('Number of Trees')\n",
    "    axes[0, 0].set_ylabel('F1 Score')\n",
    "    axes[0, 0].set_title('Effect of Number of Trees')\n",
    "\n",
    "    # Plot maxDepth\n",
    "    max_depth_df = param_metrics[param_metrics['numTrees'] == param_metrics['numTrees'].median()]\n",
    "    axes[0, 1].plot(max_depth_df['maxDepth'], max_depth_df['metric'], marker='o')\n",
    "    axes[0, 1].set_xlabel('Max Depth')\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].set_title('Effect of Max Depth')\n",
    "\n",
    "    # Plot 3D surface\n",
    "    ax = fig.add_subplot(2, 1, 2, projection='3d')\n",
    "    X = param_metrics['numTrees'].unique()\n",
    "    Y = param_metrics['maxDepth'].unique()\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    Z = param_metrics.pivot(index='maxDepth', columns='numTrees', values='metric').values\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "    ax.set_xlabel('Number of Trees')\n",
    "    ax.set_ylabel('Max Depth')\n",
    "    ax.set_zlabel('F1 Score')\n",
    "    ax.set_title('Parameter Interaction Effect on F1 Score')\n",
    "    fig.colorbar(surf)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def evaluate_model(model, data, evaluator):\n",
    "    \"\"\"Evaluate model performance using multiple metrics.\"\"\"\n",
    "    predictions = model.transform(data)\n",
    "    metrics = {}\n",
    "    for metric in [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]:\n",
    "        evaluator.setMetricName(metric)\n",
    "        metrics[metric] = evaluator.evaluate(predictions)\n",
    "    return metrics, predictions\n",
    "\n",
    "\n",
    "def create_applicant_classifier(amount_of_jobs, applicants_normalization_data):\n",
    "    # Join and prepare initial dataset\n",
    "    joined_df = amount_of_jobs.join(\n",
    "        applicants_normalization_data,\n",
    "        on=\"company\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    df_with_bins = create_binned_features(joined_df).na.drop(subset=[\"applicants_count_bin\"])\n",
    "    vector_df = prepare_features(df_with_bins)\n",
    "    # Split into train, eval, and test sets\n",
    "    train_data, eval_data, test_data = vector_df.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "    # Create evaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"applicants_count_bin\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"f1\"\n",
    "    )\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'numTrees': [50, 100, 150],\n",
    "        'maxDepth': [5, 10, 15]\n",
    "    }\n",
    "    # Train and evaluate models with different parameters\n",
    "    param_metrics = []\n",
    "    best_metric = -1\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "\n",
    "    for num_trees in param_grid['numTrees']:\n",
    "        for max_depth in param_grid['maxDepth']:\n",
    "            # Train model with current parameters\n",
    "            rf = RandomForestClassifier(\n",
    "                labelCol=\"applicants_count_bin\",\n",
    "                featuresCol=\"bin_prediction\",\n",
    "                numTrees=num_trees,\n",
    "                maxDepth=max_depth,\n",
    "                seed=42\n",
    "            )\n",
    "            model = rf.fit(train_data)\n",
    "            # Evaluate on eval set\n",
    "            eval_metrics, _ = evaluate_model(model, eval_data, evaluator)\n",
    "            # Store metrics\n",
    "            param_metrics.append({\n",
    "                'numTrees': num_trees,\n",
    "                'maxDepth': max_depth,\n",
    "                'metric': eval_metrics['f1']\n",
    "            })\n",
    "            # Update best model if necessary\n",
    "            if eval_metrics['f1'] > best_metric:\n",
    "                best_metric = eval_metrics['f1']\n",
    "                best_model = model\n",
    "                best_params = {'numTrees': num_trees, 'maxDepth': max_depth}\n",
    "\n",
    "    param_metrics_df = pd.DataFrame(param_metrics)\n",
    "\n",
    "    # Generate elbow plots\n",
    "    elbow_plot = plot_elbow_curves(param_metrics_df)\n",
    "    # Evaluate best model on test set\n",
    "    test_metrics, test_predictions = evaluate_model(best_model, test_data, evaluator)\n",
    "    # Get feature importance\n",
    "    numeric_cols = [\"record_count\", \"minutes_ago\", \"industry_cluster\"]\n",
    "    categorical_cols = [\"seniority_level\", \"employment_type\"]\n",
    "    feature_cols = numeric_cols + [f\"{col}_encoded\" for col in categorical_cols]\n",
    "\n",
    "    feature_importance = best_model.featureImportances\n",
    "    feature_importance_list = [\n",
    "        (feature, importance)\n",
    "        for feature, importance in zip(feature_cols, feature_importance)\n",
    "    ]\n",
    "    bin_distribution = test_predictions.groupBy(\"prediction\") \\\n",
    "        .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "        .orderBy(\"prediction\") \\\n",
    "        .toPandas()\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'test_metrics': test_metrics,\n",
    "        'test_predictions': test_predictions,\n",
    "        'feature_importance': feature_importance_list,\n",
    "        'bin_distribution': bin_distribution,\n",
    "        'param_metrics': param_metrics_df,\n",
    "        'elbow_plot': elbow_plot,\n",
    "        'best_params': best_params\n",
    "    }\n",
    "result = create_applicant_classifier(amount_of_jobs, applicants_normalization_data)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(f\"Number of Trees: {result['best_params']['numTrees']}\")\n",
    "print(f\"Max Depth: {result['best_params']['maxDepth']}\")\n",
    "# Print test metrics\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for metric, value in result['test_metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "# Print feature importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "for feature, importance in result['feature_importance']:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "# Print parameter tuning results\n",
    "print(\"\\nParameter Tuning Results:\")\n",
    "print(result['param_metrics'].sort_values('metric', ascending=False).head())\n",
    "\n",
    "# Display elbow plot\n",
    "plt.show()"
   ],
   "id": "b35f9d3c3bb8305d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "norm_model = result['model']\n",
    "applicants_prediction_data_for_model = create_binned_features(applicants_prediction_data)\n",
    "applicants_prediction_data_for_model = amount_of_jobs.join(\n",
    "    applicants_prediction_data_for_model,\n",
    "    on=\"company\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "applicants_prediction_data_for_model = prepare_features(applicants_prediction_data_for_model)\n",
    "applicants_prediction_data_predictions = norm_model.transform(applicants_prediction_data_for_model)\n",
    "applicants_prediction_data_predictions = applicants_prediction_data_predictions.select([\"company\", \"applicants_count\", \"description\", \"employment_type\", \"industries\", \"job_function\", \"location\", \"seniority_level\", \"industry_cluster\", \"minutes_ago\", \"title\", \"applicants_count_bin\", \"prediction\"])\n",
    "applicants_prediction_data_predictions = applicants_prediction_data_predictions.withColumnRenamed(\"prediction\", \"prediction_applicants_count_bin\")\n",
    "applicants_prediction_data_predictions = applicants_prediction_data_predictions.withColumn(\"applicant_bin_difference\", col(\"prediction_applicants_count_bin\") - col(\"applicants_count_bin\"))\n",
    "\n",
    "display(applicants_prediction_data_predictions)"
   ],
   "id": "62ece438091d74d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Show distribution of bin difference\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bin_diff_df = applicants_prediction_data_predictions.select(\"applicant_bin_difference\").toPandas()\n",
    "plt.figure(figsize=(10, 6))\n",
    "bin_diff_df['applicant_bin_difference'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.xlabel('Applicant Bin Difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Applicant Bin Difference')\n",
    "plt.figtext(0.5, -0.05, 'Note: Lower values indicate better job ads', ha='center', fontsize=10)\n",
    "plt.show()"
   ],
   "id": "ecf1d82d8ee812df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicting difference based on description and title",
   "id": "805922f86fd2a64f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, we try with the **Random Forest** model, lets investigate its performance",
   "id": "4ddf08a5fec1d8a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, udf, abs, avg, length\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"Remove null values and empty strings from the dataset\"\"\"\n",
    "    return data.filter(\n",
    "        (col(\"title\").isNotNull()) &\n",
    "        (col(\"description\").isNotNull()) &\n",
    "        (col(\"applicant_bin_difference\").isNotNull()) &\n",
    "        (length(col(\"title\")) > 0) &\n",
    "        (length(col(\"description\")) > 0)\n",
    "    )\n",
    "\n",
    "def shift_labels(data):\n",
    "    \"\"\"Shift the labels to non-negative values\"\"\"\n",
    "    return data.withColumn(\n",
    "        \"shifted_label\",\n",
    "        col(\"applicant_bin_difference\") + 8\n",
    "    )\n",
    "\n",
    "def shift_predictions_back(predictions):\n",
    "    \"\"\"Shift predictions back to original range\"\"\"\n",
    "    return predictions.withColumn(\n",
    "        \"adjusted_prediction\",\n",
    "        col(\"prediction\") - 8\n",
    "    )\n",
    "\n",
    "# Create tokenizers for both text columns\n",
    "title_tokenizer = RegexTokenizer(\n",
    "    inputCol=\"title\",\n",
    "    outputCol=\"title_words\",\n",
    "    pattern=\"\\\\W\"\n",
    ")\n",
    "\n",
    "description_tokenizer = RegexTokenizer(\n",
    "    inputCol=\"description\",\n",
    "    outputCol=\"description_words\",\n",
    "    pattern=\"\\\\W\"\n",
    ")\n",
    "\n",
    "# Remove stop words from both columns\n",
    "title_remover = StopWordsRemover(\n",
    "    inputCol=\"title_words\",\n",
    "    outputCol=\"title_filtered\"\n",
    ")\n",
    "\n",
    "description_remover = StopWordsRemover(\n",
    "    inputCol=\"description_words\",\n",
    "    outputCol=\"description_filtered\"\n",
    ")\n",
    "\n",
    "# Convert text to term frequency vectors\n",
    "title_vectorizer = CountVectorizer(\n",
    "    inputCol=\"title_filtered\",\n",
    "    outputCol=\"title_tf\",\n",
    "    minDF=5.0\n",
    ")\n",
    "\n",
    "description_vectorizer = CountVectorizer(\n",
    "    inputCol=\"description_filtered\",\n",
    "    outputCol=\"description_tf\",\n",
    "    minDF=5.0\n",
    ")\n",
    "\n",
    "# Apply IDF to both columns\n",
    "title_idf = IDF(\n",
    "    inputCol=\"title_tf\",\n",
    "    outputCol=\"title_features\"\n",
    ")\n",
    "\n",
    "description_idf = IDF(\n",
    "    inputCol=\"description_tf\",\n",
    "    outputCol=\"description_features\"\n",
    ")\n",
    "\n",
    "# Combine features from both columns\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"title_features\", \"description_features\"],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"raw_features\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Create the classifier\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"shifted_label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=100\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    title_tokenizer,\n",
    "    description_tokenizer,\n",
    "    title_remover,\n",
    "    description_remover,\n",
    "    title_vectorizer,\n",
    "    description_vectorizer,\n",
    "    title_idf,\n",
    "    description_idf,\n",
    "    assembler,\n",
    "    scaler,\n",
    "    rf\n",
    "])\n",
    "\n",
    "def train_and_evaluate_model(data):\n",
    "    \"\"\"Train and evaluate the model with data cleaning and error handling\"\"\"\n",
    "    # Print initial data count\n",
    "    initial_count = data.count()\n",
    "    print(f\"Initial number of records: {initial_count}\")\n",
    "\n",
    "    # Clean the data\n",
    "    cleaned_data = clean_data(data)\n",
    "    cleaned_count = cleaned_data.count()\n",
    "    print(f\"Number of records after cleaning: {cleaned_count}\")\n",
    "    print(f\"Removed {initial_count - cleaned_count} records with null or empty values\")\n",
    "\n",
    "    # Shift labels to non-negative range\n",
    "    shifted_data = shift_labels(cleaned_data)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data = shifted_data.randomSplit([0.8, 0.2], seed=42)\n",
    "    print(f\"Training set size: {train_data.count()}\")\n",
    "    print(f\"Test set size: {test_data.count()}\")\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Shift predictions back to original range\n",
    "    final_predictions = shift_predictions_back(predictions)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"shifted_label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Calculate mean absolute error\n",
    "    mae = final_predictions.select(\n",
    "        avg(abs(col(\"applicant_bin_difference\") - col(\"adjusted_prediction\")))\n",
    "    ).collect()[0][0]\n",
    "\n",
    "    # Calculate feature importance\n",
    "    feature_importance = model.stages[-1].featureImportances\n",
    "\n",
    "    # Show distribution of predictions\n",
    "    print(\"\\nPrediction Distribution:\")\n",
    "    final_predictions.groupBy(\"adjusted_prediction\").count().orderBy(\"adjusted_prediction\").show()\n",
    "\n",
    "    return model, accuracy, mae, feature_importance, final_predictions"
   ],
   "id": "107dda79d0ddb49a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train and evaluate the model\n",
    "model, accuracy, mae, feature_importance, predictions = train_and_evaluate_model(applicants_prediction_data_predictions)\n",
    "# Print metrics\n",
    "print(f\"\\nModel Accuracy: {accuracy}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "# Look at sample predictions with original and predicted values\n",
    "predictions.select(\n",
    "    \"title\",\n",
    "    \"applicant_bin_difference\",\n",
    "    \"adjusted_prediction\"\n",
    ").show(5)\n",
    "\n",
    "predictions.display()"
   ],
   "id": "80943fc12716a719"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the results of the random forest are not so high. so we try to investigare another model : **XGboost**",
   "id": "d2cd4be27fe07437"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, udf, abs, avg, length, count\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, udf, abs, avg, length\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"Remove null values and empty strings from the dataset\"\"\"\n",
    "    return data.filter(\n",
    "        (col(\"title\").isNotNull()) &\n",
    "        (col(\"description\").isNotNull()) &\n",
    "        (col(\"applicant_bin_difference\").isNotNull()) &\n",
    "        (length(col(\"title\")) > 0) &\n",
    "        (length(col(\"description\")) > 0)\n",
    "    )\n",
    "\n",
    "def shift_labels(data):\n",
    "    \"\"\"Shift the labels to non-negative values\"\"\"\n",
    "    return data.withColumn(\n",
    "        \"shifted_label\",\n",
    "        col(\"applicant_bin_difference\") + 8\n",
    "    )\n",
    "\n",
    "def shift_predictions_back(predictions):\n",
    "    \"\"\"Shift predictions back to original range\"\"\"\n",
    "    return predictions.withColumn(\n",
    "        \"adjusted_prediction\",\n",
    "        col(\"prediction\") - 8\n",
    "    )\n",
    "\n",
    "# Create tokenizers for both text columns\n",
    "title_tokenizer = RegexTokenizer(\n",
    "    inputCol=\"title\",\n",
    "    outputCol=\"title_words\",\n",
    "    pattern=\"\\\\W\"\n",
    ")\n",
    "\n",
    "description_tokenizer = RegexTokenizer(\n",
    "    inputCol=\"description\",\n",
    "    outputCol=\"description_words\",\n",
    "    pattern=\"\\\\W\"\n",
    ")\n",
    "\n",
    "# Remove stop words from both columns\n",
    "title_remover = StopWordsRemover(\n",
    "    inputCol=\"title_words\",\n",
    "    outputCol=\"title_filtered\"\n",
    ")\n",
    "\n",
    "description_remover = StopWordsRemover(\n",
    "    inputCol=\"description_words\",\n",
    "    outputCol=\"description_filtered\"\n",
    ")\n",
    "\n",
    "# Apply Word2Vec to both columns\n",
    "title_word2vec = Word2Vec(\n",
    "    inputCol=\"title_filtered\",\n",
    "    outputCol=\"title_features\",\n",
    "    vectorSize=100,\n",
    "    minCount=5\n",
    ")\n",
    "\n",
    "description_word2vec = Word2Vec(\n",
    "    inputCol=\"description_filtered\",\n",
    "    outputCol=\"description_features\",\n",
    "    vectorSize=100,\n",
    "    minCount=5\n",
    ")\n",
    "\n",
    "# Combine features from both columns\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"title_features\", \"description_features\"],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "# Create the XGBoost classifier\n",
    "xgb = SparkXGBClassifier(\n",
    "    label_col=\"shifted_label\",\n",
    "    features_col=\"raw_features\",\n",
    "    # objective=\"multi:softmax\",\n",
    "    num_class=16,  # Adjust based on the range of shifted labels\n",
    "    max_depth=6,\n",
    "    eta=0.1,\n",
    "    num_round=100\n",
    ")\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    title_tokenizer,\n",
    "    description_tokenizer,\n",
    "    title_remover,\n",
    "    description_remover,\n",
    "    title_word2vec,\n",
    "    description_word2vec,\n",
    "    assembler,\n",
    "    xgb\n",
    "])\n",
    "\n",
    "def tune_max_depth(train_data, eval_data, max_depth_values):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for max_depth using the elbow method\n",
    "    Returns the best max_depth value based on evaluation metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    def evaluate_max_depth(depth):\n",
    "        # Create pipeline with current max_depth\n",
    "        xgb = SparkXGBClassifier(\n",
    "            label_col=\"shifted_label\",\n",
    "            features_col=\"raw_features\",\n",
    "            num_class=16,\n",
    "            max_depth=depth,\n",
    "            eta=0.1,\n",
    "            num_round=100\n",
    "        )\n",
    "\n",
    "        current_pipeline = Pipeline(stages=[\n",
    "            title_tokenizer,\n",
    "            description_tokenizer,\n",
    "            title_remover,\n",
    "            description_remover,\n",
    "            title_word2vec,\n",
    "            description_word2vec,\n",
    "            assembler,\n",
    "            xgb\n",
    "        ])\n",
    "\n",
    "        # Train model\n",
    "        model = current_pipeline.fit(train_data)\n",
    "        predictions = model.transform(eval_data)\n",
    "\n",
    "        # Calculate metrics\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"shifted_label\",\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "        # Calculate MAE\n",
    "        final_predictions = shift_predictions_back(predictions)\n",
    "        mae = final_predictions.select(\n",
    "            avg(abs(col(\"applicant_bin_difference\") - col(\"adjusted_prediction\"))).alias(\"mae\")\n",
    "        ).collect()[0][\"mae\"]\n",
    "\n",
    "        return depth, accuracy, mae, model\n",
    "\n",
    "    # Parallel execution of hyperparameter evaluation\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        results = list(executor.map(\n",
    "            evaluate_max_depth,\n",
    "            max_depth_values\n",
    "        ))\n",
    "\n",
    "    # Manually find the best result by sorting\n",
    "    results_sorted = sorted(results, key=lambda x: x[1], reverse=True)  # Sort by accuracy\n",
    "    best_result = results_sorted[0]\n",
    "\n",
    "    return best_result\n",
    "\n",
    "\n",
    "def train_evaluate_tune_model(data):\n",
    "    \"\"\"\n",
    "    Complete pipeline for model training, tuning, and evaluation\n",
    "    \"\"\"\n",
    "    print(\"Starting model training and evaluation pipeline...\")\n",
    "\n",
    "    # Clean data\n",
    "    cleaned_data = clean_data(data)\n",
    "    print(f\"Data size after cleaning: {cleaned_data.count()}\")\n",
    "\n",
    "    # Shift labels\n",
    "    shifted_data = shift_labels(cleaned_data)\n",
    "\n",
    "    # Split data into train (60%), evaluation (20%), and test (20%) sets\n",
    "    train_data, remaining = shifted_data.randomSplit([0.6, 0.4], seed=42)\n",
    "    eval_data, test_data = remaining.randomSplit([0.5, 0.5], seed=42)\n",
    "\n",
    "    print(f\"Training set size: {train_data.count()}\")\n",
    "    print(f\"Evaluation set size: {eval_data.count()}\")\n",
    "    print(f\"Test set size: {test_data.count()}\")\n",
    "\n",
    "    # Define range of max_depth values to test\n",
    "    max_depth_values = list(range(3, 11))\n",
    "\n",
    "    print(\"\\nPerforming hyperparameter tuning...\")\n",
    "    best_depth, best_eval_accuracy, best_eval_mae, best_model = tune_max_depth(\n",
    "        train_data,\n",
    "        eval_data,\n",
    "        max_depth_values\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBest parameters found:\")\n",
    "    print(f\"max_depth: {best_depth}\")\n",
    "    print(f\"Evaluation accuracy: {best_eval_accuracy:.4f}\")\n",
    "    print(f\"Evaluation MAE: {best_eval_mae:.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_predictions = best_model.transform(test_data)\n",
    "\n",
    "    # Calculate test metrics\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"shifted_label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    test_accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "    # Shift predictions back and calculate MAE\n",
    "    final_predictions = shift_predictions_back(test_predictions)\n",
    "    test_mae = final_predictions.select(\n",
    "        avg(abs(col(\"applicant_bin_difference\") - col(\"adjusted_prediction\")))\n",
    "    ).collect()[0][0]\n",
    "\n",
    "    # Add helpful columns for analysis\n",
    "    final_predictions = final_predictions.withColumn(\n",
    "        \"prediction_difference\",\n",
    "        abs(col(\"applicant_bin_difference\") - col(\"adjusted_prediction\"))\n",
    "    ).withColumn(\n",
    "        \"prediction_correct\",\n",
    "        (col(\"applicant_bin_difference\") == col(\"adjusted_prediction\")).cast(\"integer\")\n",
    "    )\n",
    "\n",
    "    return best_model, test_accuracy, test_mae, final_predictions\n",
    "\n",
    "def analyze_predictions(predictions_df):\n",
    "    \"\"\"\n",
    "    Analyze prediction results in detail\n",
    "    \"\"\"\n",
    "    # Distribution of prediction differences\n",
    "    print(\"\\nDistribution of prediction differences:\")\n",
    "    predictions_df.groupBy(\"prediction_difference\") \\\n",
    "        .count() \\\n",
    "        .orderBy(\"prediction_difference\") \\\n",
    "        .show()\n",
    "\n",
    "    # Accuracy by original bin\n",
    "    print(\"\\nAccuracy by original bin:\")\n",
    "    predictions_df.groupBy(\"applicant_bin_difference\") \\\n",
    "        .agg(\n",
    "            avg(\"prediction_correct\").alias(\"accuracy\"),\n",
    "            count(\"*\").alias(\"count\")\n",
    "        ) \\\n",
    "        .orderBy(\"applicant_bin_difference\") \\\n",
    "        .show()\n",
    "\n",
    "    # Common misclassifications\n",
    "    print(\"\\nMost common misclassifications:\")\n",
    "    predictions_df.filter(col(\"prediction_difference\") > 0) \\\n",
    "        .groupBy(\"applicant_bin_difference\", \"adjusted_prediction\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .show(10)"
   ],
   "id": "661de3ce6e099c64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train and evaluate the model\n",
    "model, accuracy, mae, jobs_predicted = train_evaluate_tune_model(applicants_prediction_data_predictions)\n",
    "# Print metrics\n",
    "print(f\"\\nModel Accuracy: {accuracy}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "# Look at sample predictions with original and predicted values\n",
    "jobs_predicted.select(\n",
    "    \"title\",\n",
    "    \"applicant_bin_difference\",\n",
    "    \"adjusted_prediction\"\n",
    ").show(5)\n",
    "\n",
    "jobs_predicted.display()"
   ],
   "id": "73194a1176b04f89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "jobs_predicted.createOrReplaceTempView(\"jobs_predicted\")\n",
    "# Calculate the difference between the two columns\n",
    "df = spark.sql(\"SELECT *, (adjusted_prediction - applicant_bin_difference) AS difference FROM jobs_predicted\")\n",
    "# Aggregate the data to get the count of each difference value\n",
    "diff_counts = df.groupBy(\"difference\").count().orderBy(\"difference\")\n",
    "\n",
    "diff_counts_pd = diff_counts.toPandas()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(diff_counts_pd['difference'], diff_counts_pd['count'], color='blue')\n",
    "plt.xlabel('Difference (adjusted_prediction - prediction_difference)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Difference between adjusted_prediction and prediction_difference')\n",
    "plt.xticks(range(-8, 7))  # Set x-ticks from -8 to 6\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ],
   "id": "ec90e0106cd231b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Getting similar jobs\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, levenshtein, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Extract top 10 entries with the highest adjusted_prediction\n",
    "top_10_jobs = jobs_predicted.orderBy(col(\"adjusted_prediction\").desc()).limit(10)\n",
    "top_10_jobs = top_10_jobs.withColumnRenamed(\"title\", \"jobs_title\")\n",
    "top_10_jobs = top_10_jobs.withColumnRenamed(\"description\", \"jobs_description\")\n",
    "# top_10_jobs.display()\n",
    "\n",
    "# Drop entries where applicant_bin_difference is null in applicants_prediction_data_predictions\n",
    "applicants_prediction_data_predictions_adj = applicants_prediction_data_predictions.dropna(subset=[\"applicant_bin_difference\"])\n",
    "applicants_prediction_data_predictions_adj = applicants_prediction_data_predictions_adj.withColumnRenamed(\"title\", \"applicants_title\")\n",
    "applicants_prediction_data_predictions_adj = applicants_prediction_data_predictions_adj.withColumnRenamed(\"description\", \"applicants_description\")\n",
    "applicants_good = applicants_prediction_data_predictions_adj.filter(\"applicant_bin_difference <= -7\")\n",
    "applicants_bad = applicants_prediction_data_predictions_adj.filter(\"applicant_bin_difference >= 7\")\n",
    "\n",
    "# Prepare combined data for TF-IDF models\n",
    "# Combine titles from jobs and applicants\n",
    "all_titles = jobs_predicted.select(col(\"title\").alias(\"text\")).union(applicants_prediction_data_predictions.select(col(\"title\").alias(\"text\")))\n",
    "# Combine descriptions from jobs and applicants\n",
    "all_descriptions = jobs_predicted.select(col(\"description\").alias(\"text\")).union(applicants_prediction_data_predictions.select(col(\"description\").alias(\"text\")))\n",
    "\n",
    "# Define pipelines for title and description processing\n",
    "title_pipeline = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"tokens\"),\n",
    "    StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\"),\n",
    "    CountVectorizer(inputCol=\"filtered\", outputCol=\"tf\"),\n",
    "    IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "])\n",
    "\n",
    "desc_pipeline = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"tokens\"),\n",
    "    StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\"),\n",
    "    CountVectorizer(inputCol=\"filtered\", outputCol=\"tf\"),\n",
    "    IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "])\n",
    "\n",
    "# Fit models on combined data\n",
    "title_model = title_pipeline.fit(all_titles)\n",
    "desc_model = desc_pipeline.fit(all_descriptions)\n",
    "\n",
    "# Join the top 10 jobs with applicants\n",
    "cross_joined_good_df = top_10_jobs.alias(\"jobs\").crossJoin(applicants_good.alias(\"applicants\"))\n",
    "cross_joined_bad_df = top_10_jobs.alias(\"jobs\").crossJoin(applicants_bad.alias(\"applicants\"))\n",
    "select_cols = [\"jobs.company\", \"jobs.applicants_count\", \"jobs_title\", \"jobs_description\", \"jobs.applicant_bin_difference\", \"jobs.adjusted_prediction\", \"applicants.company\", \"applicants.applicants_count\", \"applicants_title\", \"applicants_description\", \"applicants.applicant_bin_difference\"]\n",
    "jobs_col = [\"jobs.company\", \"jobs.applicants_count\", \"jobs_title\", \"jobs_description\", \"jobs.applicant_bin_difference\", \"jobs.adjusted_prediction\"]\n",
    "cross_joined_good_df = cross_joined_good_df.select(select_cols)\n",
    "cross_joined_bad_df = cross_joined_bad_df.select(select_cols)\n",
    "\n",
    "# Define a UDF to calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return float(v1.dot(v2) / (v1.norm(2) * v2.norm(2)))\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "# Process titles and descriptions using the fitted models\n",
    "def apply_title_model(df, model, col_name):\n",
    "    df = df.withColumnRenamed(col_name, \"text\")\n",
    "    df = model.transform(df)\n",
    "    df = df.withColumnRenamed(\"text\", col_name)\n",
    "    df = df.withColumnRenamed(\"tfidf\", f\"{col_name}_tfidf\")\n",
    "    return df.drop(\"tokens\", \"filtered\", \"tf\")\n",
    "\n",
    "def apply_desc_model(df, model, col_name):\n",
    "    df = df.withColumnRenamed(col_name, \"text\")\n",
    "    df = model.transform(df)\n",
    "    df = df.withColumnRenamed(\"text\", col_name)\n",
    "    df = df.withColumnRenamed(\"tfidf\", f\"{col_name}_tfidf\")\n",
    "    return df.drop(\"tokens\", \"filtered\", \"tf\")\n",
    "\n",
    "# Apply title model to jobs_title and applicants_title\n",
    "cross_joined_good_df = apply_title_model(cross_joined_good_df, title_model, \"jobs_title\")\n",
    "cross_joined_good_df = apply_title_model(cross_joined_good_df, title_model, \"applicants_title\")\n",
    "\n",
    "# Apply description model to jobs_description and applicants_description\n",
    "cross_joined_good_df = apply_desc_model(cross_joined_good_df, desc_model, \"jobs_description\")\n",
    "cross_joined_good_df = apply_desc_model(cross_joined_good_df, desc_model, \"applicants_description\")\n",
    "\n",
    "# Calculate cosine similarity for titles and descriptions\n",
    "cross_joined_good_df = cross_joined_good_df.withColumn(\n",
    "    \"title_similarity\",\n",
    "    cosine_similarity_udf(col(\"jobs_title_tfidf\"), col(\"applicants_title_tfidf\"))\n",
    ").withColumn(\n",
    "    \"description_similarity\",\n",
    "    cosine_similarity_udf(col(\"jobs_description_tfidf\"), col(\"applicants_description_tfidf\"))\n",
    ")\n",
    "# Combine similarities\n",
    "cross_joined_good_df = cross_joined_good_df.withColumn(\n",
    "    \"combined_similarity\",\n",
    "    col(\"title_similarity\") * 0.4 + col(\"description_similarity\") * 0.6\n",
    ")\n",
    "# Select top entry for each job\n",
    "window_spec = Window.partitionBy(\"jobs_title\").orderBy(col(\"combined_similarity\").desc())\n",
    "cross_joined_good_df = cross_joined_good_df.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Repeat processing for bad applicants\n",
    "cross_joined_bad_df = apply_title_model(cross_joined_bad_df, title_model, \"jobs_title\")\n",
    "cross_joined_bad_df = apply_title_model(cross_joined_bad_df, title_model, \"applicants_title\")\n",
    "cross_joined_bad_df = apply_desc_model(cross_joined_bad_df, desc_model, \"jobs_description\")\n",
    "cross_joined_bad_df = apply_desc_model(cross_joined_bad_df, desc_model, \"applicants_description\")\n",
    "cross_joined_bad_df = cross_joined_bad_df.withColumn(\n",
    "    \"title_similarity\",\n",
    "    cosine_similarity_udf(col(\"jobs_title_tfidf\"), col(\"applicants_title_tfidf\"))\n",
    ").withColumn(\n",
    "    \"description_similarity\",\n",
    "    cosine_similarity_udf(col(\"jobs_description_tfidf\"), col(\"applicants_description_tfidf\"))\n",
    ").withColumn(\n",
    "    \"combined_similarity\",\n",
    "    col(\"title_similarity\") * 0.4 + col(\"description_similarity\") * 0.6\n",
    ")\n",
    "window_spec_bad = Window.partitionBy(\"jobs_title\").orderBy(col(\"combined_similarity\").desc())\n",
    "cross_joined_bad_df = cross_joined_bad_df.withColumn(\"rank\", row_number().over(window_spec_bad)).filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Display results\n",
    "cross_joined_good_df = cross_joined_good_df.select(select_cols)\n",
    "cross_joined_bad_df = cross_joined_bad_df.select(select_cols)\n",
    "display(cross_joined_good_df) # An input with a good similar match\n",
    "display(cross_joined_bad_df) # An input with a bad similar match"
   ],
   "id": "3cc76b2cb96ab2ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyze a good description structure",
   "id": "991e75da6fc512c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate the mean length and common length good description has",
   "id": "8d2eeb9531fa4632"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import length, col\n",
    "\n",
    "# Add a new column for description length\n",
    "applicants_good = applicants_good.withColumn(\"description_length\", length(col(\"applicants_description\")))\n",
    "# Calculate statistics for the description length\n",
    "applicants_good.selectExpr(\n",
    "    \"AVG(description_length) as avg_length\",\n",
    "    \"MIN(description_length) as min_length\",\n",
    "    \"MAX(description_length) as max_length\",\n",
    "    \"STDDEV(description_length) as stddev_length\"\n",
    ").show()\n",
    "# Distribution of lengths\n",
    "length_distribution = applicants_good.groupBy(\"description_length\").count()\n",
    "length_distribution.orderBy(\"description_length\").show(10, truncate=False)\n"
   ],
   "id": "44e2c2b604d8ac2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the distribution of description length\n",
    "length_data = applicants_good.select(\"description_length\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(length_data[\"description_length\"], bins=20, edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Distribution of Description Lengths\", fontsize=16)\n",
    "plt.xlabel(\"Description Length\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9f5852ca5c8ee6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analyze the common words in a good description",
   "id": "17eb624a3207bef7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import explode, split, lower, regexp_replace, col\n",
    "\n",
    "# Preprocess descriptions: clean and split into words\n",
    "applicants_good = applicants_good.withColumn(\n",
    "    \"words\",\n",
    "    split(\n",
    "        regexp_replace(lower(col(\"applicants_description\")), \"[^a-zA-Z\\\\s]\", \"\"),\n",
    "        \"\\\\s+\"\n",
    "    )\n",
    ")\n",
    "# Remove stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "applicants_good = remover.transform(applicants_good)\n",
    "word_df = applicants_good.select(explode(col(\"filtered_words\")).alias(\"word\"))\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = word_df.groupBy(\"word\").count()\n",
    "word_freq = word_freq.orderBy(col(\"count\").desc())\n",
    "# Show the top 10 most common words and their count\n",
    "word_freq.show(10, truncate=False)"
   ],
   "id": "323b65a1b0417f69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Introduce word cloud of most common words",
   "id": "e4f9f2dca432eba9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_freq_pd = word_freq.toPandas()\n",
    "word_dict = dict(zip(word_freq_pd[\"word\"], word_freq_pd[\"count\"]))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_dict)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Most Common Words\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d2462a0840dd8d2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
